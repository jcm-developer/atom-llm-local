# ğŸ¤– Atom LLM Local

Intelligent chat system with advanced document generation and data visualization capabilities.

## âœ¨ Key Features

- ğŸ’¬ **Intelligent Chat**: Conversational interface with local AI and ChatGPT
- ğŸ“Š **Chart Generation**: Automatically creates data visualizations (bar, line, pie, scatter)
- ğŸ“„ **PDF Generation**: Generates professionally formatted PDF documents
- ğŸ”„ **Model Switching**: Toggle between private local model and ChatGPT
- ğŸ¨ **Modern Interface**: Elegant design with glass effects and gradients
- ğŸ–¼ï¸ **Image Visualization**: Full-screen preview with interactive modal

## ğŸ“‚ Project Structure

```
atom-llm-local/
â”œâ”€â”€ backend/              # Backend API (FastAPI + Python)
â”‚   â”œâ”€â”€ main.py          # Main server with endpoints
â”‚   â”œâ”€â”€ tools/           # Generation tools
â”‚   â”‚   â”œâ”€â”€ generate_pdf.py      # PDF generator
â”‚   â”‚   â””â”€â”€ generate_chart.py    # Chart generator
â”‚   â”œâ”€â”€ files/           # Generated files (PDFs, images)
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ Dockerfile       # Backend Docker image
â”œâ”€â”€ frontend/            # Frontend (Vue.js + Vite)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ views/       # Main views
â”‚   â”‚   â”œâ”€â”€ components/  # Vue components
â”‚   â”‚   â””â”€â”€ style.css    # Global styles
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ docker-compose.yml   # Service orchestration
â””â”€â”€ .env                # Environment variables

```

## ğŸ“‹ Requirements

- ğŸ³ **Docker** and **Docker Compose** (recommended)
- ğŸ“¦ **Node.js** v20+ and npm (for local development)
- ğŸ **Python** 3.9+ (for backend local development)
- ğŸŒ **Anything LLM** (gateway to local model)
- ğŸ–¥ï¸ **LM Studio** (local inference server)

## âš™ï¸ Installation and Setup

### 1. Clone the repository

```bash
git clone https://github.com/jcm-developer/atom-llm-local.git
cd atom-llm-local
```

### 2. Configure environment variables

Create a `.env` file in the project root:

```env
# API Keys
OPENAI_API_KEY=your_openai_api_key
ANYTHING_LLM_API_KEY=your_anything_llm_api_key

# URLs
ANYTHING_LLM_URL=http://localhost:3001
```

### 3. Start with Docker (Recommended)

```bash
# Start the backend
docker-compose up -d backend

# Verify it's running
docker logs backend

# Backend will be available at http://localhost:8000
```

### 4. Start the frontend

```bash
cd frontend
npm install
npm run dev

# El frontend estarÃ¡ disponible en http://localhost:5173
```

## ğŸ”„ Flujo de Trabajo

```
Usuario â†’ Frontend (Vue.js) â†’ Backend (FastAPI) â†’ Anything LLM â†’ LM Studio
                    â†“
              GeneraciÃ³n de PDFs
              GeneraciÃ³n de GrÃ¡ficas
```

### Arquitectura de Servicios

1. **Frontend (Puerto 5173)**: Interfaz de usuario en Vue.js
2. **Backend (Puerto 8000)**: API REST con FastAPI
3. **Anything LLM (Puerto 3001)**: Gateway hacia el modelo local
4. **LM Studio**: Model inference server

## ğŸ¯ Detailed Features

### 1. ğŸ’¬ Intelligent Chat

**Features:**
- Natural conversation with AI
- Conversation history
- Markdown format in responses
- Animated typing indicator

**Usage:**
```
User: "Tell me about AI"
Bot: [Model response...]
```

### 2. ğŸ“Š GeneraciÃ³n de GrÃ¡ficas

**Tipos de grÃ¡ficas disponibles:**
- ğŸ“Š **Barras**: Comparaciones entre categorÃ­as
- ğŸ“ˆ **LÃ­neas**: EvoluciÃ³n temporal
- ğŸ¥§ **Circular**: DistribuciÃ³n porcentual
- ğŸ“ **DispersiÃ³n**: Correlaciones de datos

**Comandos:**
```
"Genera una grÃ¡fica de barras de los ingresos por aÃ±o"
"Crea un grÃ¡fico circular de las ventas por producto"
"Muestra una grÃ¡fica de lÃ­neas de la evoluciÃ³n mensual"
```

**Formato de datos:**
El modelo debe responder con JSON:
```json
{
  "2020": 50000000,
  "2021": 55000000,
  "2022": 60000000,
  "2023": 58000000
}
```

**Features:**
- âœ… Inline visualization in chat
- âœ… Click to enlarge in modal
- âœ… Styled download button
- âœ… High resolution (300 DPI)
- âœ… Professional styles with matplotlib

### 3. ğŸ“„ PDF Generation

**Commands:**
```
"Generate a PDF about climate change"
"Create a PDF document of the annual report"
```

**Features:**
- âœ… Professional format with margins
- âœ… Standard A4 size
- âœ… Readable fonts (11pt)
- âœ… Automatic justification
- âœ… Styled download button

**Chat message:**
```
Here is your PDF
[ğŸ“¥ Download PDF]
```

### 4. ğŸ”„ Model Switching

**Available models:**

| Model | Icon | Description | Functions |
|-------|------|-------------|-----------|
| **Local** ğŸ’» | `computer` | Private model (LM Studio) | Chat + PDFs + Charts |
| **ChatGPT** â˜ï¸ | `cloud` | OpenAI API | Chat only |

**How to switch:**
1. Click the toggle button (input corner)
2. **Blue** icon = ChatGPT active
3. **Gray** icon = Local model active

## ğŸ¨ User Interface

### Visual Elements

**Download Buttons:**
- Blue-purple gradient (#1d7efd â†’ #8f6fff)
- Material Icons icon
- Hover effect with elevation
- Soft shadows

**Image Modal:**
- Transparent black background (90%)
- Click outside to close
- Smooth animations (fadeIn/zoomIn)
- Responsive centered image

**Theme:**
- Dark mode by default
- Glass effects (backdrop-filter)
- Consistent colors
- Poppins typography

## ğŸ› ï¸ Useful Commands

### Backend (Docker)

```bash
# Start backend
docker-compose up -d backend

# View logs in real-time
docker logs -f backend

# Stop backend
docker-compose down

# Rebuild after changes
docker-compose up -d --build backend

# Restart backend
docker-compose restart backend
```

### Frontend

```bash
# Install dependencies
npm install

# Development with hot-reload
npm run dev

# Build for production
npm run build

# Preview build
npm run preview
```

### Python (Local Development)

```bash
# Activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r backend/requirements.txt

# Run server
cd backend
python main.py
```

## ğŸŒ Environment Variables

### Backend (.env in root)

```env
# OpenAI API
OPENAI_API_KEY=sk-...                          # OpenAI API key

# Anything LLM
ANYTHING_LLM_URL=http://localhost:3001         # Gateway URL
ANYTHING_LLM_API_KEY=ABC123...                 # Anything LLM API key
```

### Frontend (frontend/.env)

```env
# Only needed if using ChatGPT
VITE_OPENAI_API_KEY=sk-...
```

**Note:** `VITE_*` variables are injected at build time. If you change these variables, you need to rebuild the frontend.

## ğŸ“¡ API Endpoints

### Backend (Port 8000)

#### POST `/api/chat`
Main endpoint for chat and content generation.

**Request:**
```json
{
  "message": "Generate a sales chart",
  "isUsingChatGPT": false
}
```

**Response (Text):**
```json
{
  "type": "text",
  "response": "Here is the information..."
}
```

**Response (Image):**
```json
{
  "type": "image",
  "filename": "chart_bar_sales_1234567890.png",
  "imageData": "data:image/png;base64,...",
  "url": "http://localhost:8000/files/chart_bar_sales_1234567890.png",
  "message": "Bar chart generated successfully"
}
```

**Response (PDF):**
```json
{
  "type": "file",
  "filename": "document_report_1234567890.pdf",
  "url": "http://localhost:8000/files/document_report_1234567890.pdf",
  "message": "PDF generated successfully"
}
```

#### GET `/files/{filename}`
Download generated files (PDFs or images).

**Response:**
- Content-Type: `application/pdf` or `image/png`
- Content-Disposition: `attachment; filename={filename}`

## ğŸ” Command Detection

### Chart Generation

**Keywords:**
- Action: `generar`, `genera`, `crear`, `crea`, `generate`, `create`
- Object: `grÃ¡fica`, `grafica`, `grÃ¡fico`, `grafico`, `chart`

**Detected types:**
- `lÃ­nea`, `linea`, `line` â†’ Line chart
- `circular`, `pie`, `pastel` â†’ Pie chart
- `dispersiÃ³n`, `dispersion`, `scatter` â†’ Scatter plot
- Default â†’ Bar chart

**Examples:**
```
âœ… "Generate a chart of revenue"
âœ… "Create a pie chart of sales"
âœ… "Show a line chart of monthly data"
```

### PDF Generation

**Keywords:**
- Action: `generar`, `genera`, `generate`
- Object: `pdf`

**Example:**
```
âœ… "Generate a PDF about the topic"
âœ… "Generate a PDF document of the report"
```

## ğŸ› Troubleshooting

### Error: "Failed to fetch"

**Cause:** Backend is not running.

**Solution:**
```bash
docker-compose up -d backend
docker logs backend  # Verify it's running
```

### Error: "Python not found"

**Cause:** Trying to run backend without Docker.

**Solution:**
```bash
# Use Docker (recommended)
docker-compose up -d backend

# Or install Python and dependencies
pip install -r backend/requirements.txt
```

### Chart not generating correctly

**Cause:** Model is not responding in JSON format.

**Solution:**
- Verify the model responds with `{"key": value}`
- Check backend logs: `docker logs -f backend`
- Example of correct response:
  ```json
  {"2020": 50000, "2021": 55000, "2022": 60000}
  ```

### PDF downloads automatically

**Cause:** Old code version.

**Solution:**
- Update repository: `git pull`
- Rebuild backend: `docker-compose up -d --build backend`

### Image not showing in chat

**Cause:** Response type is not `"image"`.

**Solution:**
- Check backend response in browser console
- Should be `data.type === 'image'` with `data.imageData`

## ğŸ“¦ Main Dependencies

### Backend (Python)

```txt
fastapi==0.104.1        # Web framework
uvicorn==0.24.0        # ASGI server
python-dotenv==1.0.0   # Environment variables
requests==2.31.0       # HTTP client
reportlab==4.0.7       # PDF generation
matplotlib==3.8.2      # Chart generation
pandas==2.1.4          # Data processing
mcp==1.0.0             # Model Context Protocol
```

### Frontend (Node.js)

```json
{
  "vue": "^3.4.0",
  "vite": "^5.0.0"
}
```

## ğŸš€ Roadmap

- [ ] Support for more chart types (histograms, box plots)
- [ ] Export charts in multiple formats (SVG, JPG)
- [ ] PDF editor with customizable templates
- [ ] Persistent conversation history
- [ ] Configurable dark/light mode
- [ ] Internationalization (i18n)
- [ ] Unit and integration tests

## ğŸ“ License

This project is licensed under the license specified in the `LICENSE` file.

## ğŸ‘¨â€ğŸ’» Author

**JCM Developer**
- GitHub: [@jcm-developer](https://github.com/jcm-developer)

## ğŸ¤ Contributing

Contributions are welcome. Please:

1. Fork the project
2. Create a branch for your feature (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ Support

If you encounter any issues or have questions:
- Open an [Issue](https://github.com/jcm-developer/atom-llm-local/issues)
- Check the [Troubleshooting](#-troubleshooting) section

---

â­ If this project has been useful to you, consider giving it a star on GitHub!
